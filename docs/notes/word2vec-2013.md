# Reading Notes

Before the Word2Vec paper, language models mainly relied on the Feedforward Neural Network Language Model (NNLM), which includes embedding, a hidden layer, and a softmax output layer, and the Recurrent Neural Network Language Model (RNNLM), which uses a hidden recurrent layer followed by softmax. However, these architectures require significant computational cost.

The Word2Vec paper removes the hidden layer, which greatly reduces computational complexity and allows training on much larger datasets. According to the experimental results and model architecture, semantically similar words tend to align in similar vector directions, revealing linear relationships in the embedding space.

---

# Learning From AI (GPT)

### 1ï¸âƒ£ æœ¬æ–‡çœŸæ­£ç›®æ¨™ä¸æ˜¯èªè¨€æ¨¡å‹ï¼Œè€Œæ˜¯é«˜å“è³ªè©å‘é‡

æ ¸å¿ƒï¼šrepresentation learningï¼Œä¸æ˜¯ perplexity æœ€ä½³åŒ–ã€‚

ä¾æ“šï¼š
- Abstractï¼šlearn high-quality word vectors
- Sec 1.1ï¼šç›®æ¨™æ˜¯ preserve linear regularities

ğŸ‘‰ ä»–å€‘åˆ»æ„çŠ§ç‰²èªè¨€æ¨¡å‹è¤‡é›œåº¦ï¼Œæ›å– embedding å¹¾ä½•å“è³ªã€‚

### 2ï¸âƒ£ è¨ˆç®—è¤‡é›œåº¦æ˜¯è¨­è¨ˆå‡ºç™¼é»

è¨“ç·´æˆæœ¬ï¼šO = E Ã— T Ã— Qï¼ˆä¾æ“š Sec 2ï¼‰

NNLM çš„ç“¶é ¸ï¼šQ = NÃ—D + NÃ—DÃ—H + HÃ—Vï¼ˆä¾æ“š Eq.(2)ï¼‰

ğŸ‘‰ ç§»é™¤ hidden layer æ˜¯ç‚ºäº†é™è¤‡é›œåº¦ã€‚

### 3ï¸âƒ£ CBOW vs Skip-gram çš„æ ¹æœ¬å·®ç•°

- CBOWï¼šå¹³å‡ context â†’ é æ¸¬ä¸­å¿ƒè©
- Skip-gramï¼šä¸­å¿ƒè© â†’ é æ¸¬æ¯å€‹ context

ä¾æ“š Sec 3.1, 3.2

ğŸ‘‰ Skip-gram å° semantic é¡æ¯”æ›´å¼·ï¼ˆTable 3ï¼‰ï¼ŒåŸå› ï¼šå°å…±ç¾å·®ç•°å»ºæ¨¡æ›´ç²¾ç´°ã€‚

### 4ï¸âƒ£ ç·šæ€§èªæ„çµæ§‹ä¸æ˜¯å¶ç„¶

ä¾‹å­ï¼šking âˆ’ man + woman â‰ˆ queenï¼ˆä¾æ“š Sec 1.1ï¼‰

ğŸ‘‰ èªæ„æ–¹å‘ä¾†è‡ª log å…±ç¾æ©Ÿç‡å·®ç•°ã€‚

### 5ï¸âƒ£ æ¨¡å‹æœ¬è³ªæ˜¯ log-linear model

Sec 3 é–‹é ­æ˜ç¢ºæŒ‡å‡ºæ˜¯ log-linearï¼Œä¹Ÿå°±æ˜¯ï¼š

v_w Â· v_c â‰ˆ log P(c|w)

ğŸ‘‰ é€™ç­‰åƒ¹æ–¼ä½ç§©çŸ©é™£åˆ†è§£ã€‚

### 6ï¸âƒ£ Hierarchical Softmax æ˜¯æ•ˆç‡é—œéµ

ç”¨ Huffman tree æŠŠ softmax å¾ V é™ç‚º log(V)ï¼ˆä¾æ“š Sec 2.1ï¼‰

ğŸ‘‰ æ²’é€™å€‹æŠ€å·§ï¼Œ1M vocab ä¸å¯èƒ½è¨“ç·´ã€‚

### 7ï¸âƒ£ window æ˜¯çµ±è¨ˆç¯„åœæ§åˆ¶å™¨

Skip-gram ç”¨éš¨æ©Ÿ R âˆˆ [1, C]ï¼ˆä¾æ“š Sec 3.2ï¼‰

ğŸ‘‰ å° window â†’ å±€éƒ¨èªæ„
ğŸ‘‰ å¤§ window â†’ ä¸»é¡Œèªæ„

é€™å½±éŸ¿ embedding å¹¾ä½•ã€‚

### 8ï¸âƒ£ è³‡æ–™é‡èˆ‡ç¶­åº¦å¿…é ˆåŒ¹é…

Sec 4.2ï¼ˆTable 2ï¼‰è§€å¯Ÿï¼š
- åªå¢åŠ ç¶­åº¦ â†’ é‚Šéš›æ”¶ç›Šéæ¸›
- åªå¢åŠ è³‡æ–™ â†’ é‚Šéš›æ”¶ç›Šéæ¸›
- åŒæ™‚å¢åŠ  â†’ æ•ˆæœæœ€å¥½

ğŸ‘‰ bias-variance + capacity matchingã€‚

### 9ï¸âƒ£ è©•ä¼°æ–¹å¼æ˜¯å¹¾ä½•æ¸¬è©¦ï¼Œä¸æ˜¯ç”Ÿæˆæ¸¬è©¦

Semantic-Syntactic testï¼ˆSec 4.1ï¼‰

é€™ä¸æ˜¯ P(sentence)ï¼Œè€Œæ˜¯ï¼šå‘é‡å·® + cosine è·é›¢ã€‚

ğŸ‘‰ æª¢é©— embedding ç©ºé–“çµæ§‹ã€‚

### ğŸ”Ÿ çœŸæ­£æ·±å±¤æ´å¯Ÿï¼šèªè¨€å…±ç¾çµ±è¨ˆå…·æœ‰ä½ç§©çµæ§‹

å³ä½¿æ¨¡å‹è®Šå¤§ï¼ˆå¦‚ Transformerï¼‰ï¼Œèªæ„æ–¹å‘ä»ç„¶å­˜åœ¨ã€‚

é€™èªªæ˜ï¼š
- ç·šæ€§æ–¹å‘ä¸æ˜¯æ¨¡å‹å‰¯ç”¢å“
- è€Œæ˜¯èªè¨€çµ±è¨ˆæœ¬èº«å…·æœ‰ä½ç§©å¹¾ä½•

é€™é»é›–æœªæ˜èªªï¼Œä½†å¾æ•´é«”çµæœï¼ˆSec 4ï¼‰å¯æ¨è«–ã€‚

### ç¸½çµä¸€å¥è©±

Word2Vec çš„æˆåŠŸä¸åœ¨æ–¼æ¨¡å‹è¤‡é›œï¼Œè€Œåœ¨æ–¼ï¼šç”¨æ¥µç°¡ç·šæ€§æ¨¡å‹ + å¤§é‡è³‡æ–™ï¼Œæ¢å¾©èªè¨€å…±ç¾çŸ©é™£çš„ä½ç§©å¹¾ä½•çµæ§‹ã€‚

**ChatGPT å°è©±é€£çµ**: https://chatgpt.com/share/69995667-bc04-800a-9893-0dfac0072eaf