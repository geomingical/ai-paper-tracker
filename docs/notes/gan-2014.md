# Reading Notes

In 2012, AlexNet marked a major milestone for discriminative models by achieving strong performance on image classification with deep convolutional networks. However, a good classifier does not necessarily learn how the data are generated or how to produce realistic images. GAN (Generative Adversarial Networks) is an early attempt to address this by training a generative model without explicitly computing likelihoods.

In the original GAN paper, both the generator (G) and the discriminator (D) can be implemented as multilayer perceptrons (MLPs). The discriminator is a binary classifier that outputs D(x), the probability that an input image x comes from the real data rather than from the generator. Its training objective has two parts: one term encourages high D(x) for real images, and the other encourages low D(G(z)) for generated (fake) images. These two terms are used to update the discriminator's parameters.

The generator is updated using gradients that flow through the discriminator, but the discriminator's parameters are not updated during the generator step. The generator's goal is to produce images that make the discriminator output a high probability of being real (i.e., to "fool" the discriminator). Importantly, the generator's objective is not simply the negative of the discriminator's fake-image loss; instead, it is designed to push D(G(z)) upward, often using the non-saturating objective for stronger gradients early in training.

**Reference**: https://aws.amazon.com/tw/what-is/gan/

---

# Learning From AI (GPT5.2)

## GAN è«–æ–‡ Reading Notesï¼ˆ10 é»ç‰ˆï¼‰

### 1ï¸âƒ£ GAN çš„å‹•æ©Ÿä¸æ˜¯è³‡æ–™ä¸å¤ ï¼Œè€Œæ˜¯ã€Œç”Ÿæˆæ¨¡å‹è¨“ç·´å¾ˆé›£ã€

åœ¨ GAN å‡ºç¾å‰ï¼Œç”Ÿæˆæ¨¡å‹ï¼ˆRBMã€DBMã€MLEï¼‰ä¸æ˜¯ç®—ä¸å‹•ï¼Œå°±æ˜¯è¨“ç·´ä¸ç©©ã€‚
GAN çš„æ ¸å¿ƒå‹•æ©Ÿæ˜¯ï¼šèƒ½ä¸èƒ½é¿é–‹é¡¯å¼æ©Ÿç‡è¨ˆç®—ï¼Œåªç”¨ backprop å­¸åˆ†ä½ˆï¼Ÿ

### 2ï¸âƒ£ åˆ¤åˆ¥æ¨¡å‹æˆåŠŸ â‰  ç†è§£è³‡æ–™åˆ†ä½ˆ

AlexNet é¡æ¨¡å‹åªéœ€è¦å­¸ã€Œæ±ºç­–é‚Šç•Œã€ï¼Œä¸éœ€è¦ç†è§£è³‡æ–™å…§éƒ¨çµæ§‹ã€‚
ç”Ÿæˆæ¨¡å‹çš„é‡è¦æ€§ä¾†è‡ªä¸€å€‹ä¿¡å¿µï¼šå¦‚æœæ¨¡å‹çœŸçš„æ‡‚è³‡æ–™ï¼Œå®ƒæ‡‰è©²èƒ½ç”Ÿæˆè³‡æ–™ã€‚

### 3ï¸âƒ£ GAN çš„æ ¸å¿ƒå‰µæ–°æ˜¯ã€Œè¨“ç·´æ–¹å¼ã€ï¼Œä¸æ˜¯ç¶²è·¯æ¶æ§‹

Generator å’Œ Discriminator æœ¬èº«åªæ˜¯æ™®é€šç¥ç¶“ç¶²è·¯ã€‚
çœŸæ­£çš„æ–°æ±è¥¿æ˜¯ï¼šç”¨ä¸€å€‹å¯å­¸çš„å°æ‰‹ï¼ˆDï¼‰ä¾†æä¾›ç”Ÿæˆæ¨¡å‹ï¼ˆGï¼‰çš„è¨“ç·´è¨Šè™Ÿã€‚

### 4ï¸âƒ£ Discriminator ä¸æ˜¯ç›®æ¨™æ¨¡å‹ï¼Œè€Œæ˜¯ã€Œæ¢¯åº¦ç”¢ç”Ÿå™¨ã€

D çš„è§’è‰²ä¸æ˜¯è®Šæˆæœ€å¼·åˆ†é¡å™¨ï¼Œè€Œæ˜¯æä¾›å° G æœ‰ç”¨çš„æ¢¯åº¦ã€‚
ç†æƒ³ç‹€æ…‹ä¸‹ï¼ŒD åè€Œæœƒè¼¸å‡ºæ¥è¿‘ 0.5ï¼ˆä»£è¡¨çœŸå‡é›£åˆ†ï¼‰ã€‚

### 5ï¸âƒ£ Generator å¾é ­åˆ°å°¾æ²’æœ‰ã€Œçœ‹éè³‡æ–™ã€

G ä¸æ¥è§¸çœŸå¯¦åœ–ç‰‡ã€ä¸ç”¨åƒç´ èª¤å·®ã€ä¸ç”¨ labelã€‚
å®ƒå”¯ä¸€çš„å­¸ç¿’ä¾†æºæ˜¯ï¼šD å°å‡åœ–çš„åæ‡‰ã€‚
â†’ é€™ç›´æ¥è§£é‡‹äº†ç‚ºä»€éº¼ GAN å®¹æ˜“ä¸ç©©å®šã€‚

### 6ï¸âƒ£ GAN loss çš„å•é¡Œä¸æ˜¯æ–¹å‘éŒ¯ï¼Œè€Œæ˜¯ã€Œæ¢¯åº¦å“è³ªä¸å¥½ã€

åŸå§‹ minimax loss åœ¨è¨“ç·´æ—©æœŸæœƒæ¢¯åº¦æ¶ˆå¤±ï¼ˆD å¤ªå¼·ï¼‰ã€‚
æ”¹ç”¨ non-saturating loss ä¸¦ä¸æ˜¯æ”¹ç›®æ¨™ï¼Œè€Œæ˜¯è®“æ¢¯åº¦åœ¨æ—©æœŸé‚„æ´»è‘—ã€‚

### 7ï¸âƒ£ Mode collapse ä¸æ˜¯ bugï¼Œè€Œæ˜¯æ¢¯åº¦çš„å¿…ç„¶çµæœ

GAN çš„ loss åªé¼“å‹µã€Œåƒä¸åƒçœŸã€ï¼Œå®Œå…¨ä¸é¼“å‹µã€Œå½¼æ­¤è¦ä¸åŒã€ã€‚
ç•¶æŸå€‹è¼¸å‡ºç‰¹åˆ¥å¥½é¨™ï¼Œæ¢¯åº¦æœƒæŠŠæ‰€æœ‰ z æ¨å‘åŒä¸€å€‹åœ°æ–¹ã€‚

### 8ï¸âƒ£ Helvetica scenario æ˜¯ mode collapse çš„æ¥µç«¯ä¾‹å­

Generator æ‰¾åˆ°ä¸€å€‹ã€Œå®‰å…¨ä½†ä¸­åº¸ã€çš„è¼¸å‡ºæ¨¡æ¿ï¼Œ
å° D ä¾†èªªä¸æ˜é¡¯å‡ï¼Œä½†å°åˆ†ä½ˆä¾†èªªæ˜¯ç½é›£ã€‚
â†’ loss çœ‹èµ·ä¾†å¾ˆå¥½ï¼Œä½†æ¨¡å‹å¯¦éš›ä¸Šå¤±æ•—ã€‚

### 9ï¸âƒ£ WGANã€Diffusionã€Transformer éƒ½åœ¨è§£åŒä¸€å€‹å•é¡Œ

å•é¡Œå¾ä¾†ä¸æ˜¯ã€Œæ¨¡å‹å¤ ä¸å¤ å¼·ã€ï¼Œè€Œæ˜¯ï¼š
åœ¨æœ€å£æƒ…æ³ä¸‹ï¼Œæ¢¯åº¦æ˜¯ä¸æ˜¯ä»ç„¶æœ‰æ–¹å‘ã€æœ‰å¤§å°ã€å¯ç”¨ï¼Ÿ
é€™æ˜¯ GAN â†’ WGAN â†’ Diffusion â†’ å¤§å‹ Transformer çš„å…±åŒä¸»ç·šã€‚

### ğŸ”Ÿ GAN æ˜¯ä¸€å€‹ã€Œæ€æƒ³èµ·é»ã€ï¼Œä¸æ˜¯æœ€çµ‚ç­”æ¡ˆ

åŸå§‹ GAN è«–æ–‡çš„åƒ¹å€¼ï¼Œä¸åœ¨æ–¼å®ƒæ˜¯å¦ç©©å®šã€æ˜¯å¦æœ€å¥½ç”¨ï¼Œ
è€Œåœ¨æ–¼å®ƒé‡æ–°å®šç¾©äº†ã€Œç”Ÿæˆæ¨¡å‹å¯ä»¥æ€éº¼è¢«è¨“ç·´ã€ã€‚
å¾ŒçºŒæ‰€æœ‰ç”Ÿæˆæ¨¡å‹ï¼Œéƒ½åœ¨å›æ‡‰å®ƒæš´éœ²å‡ºçš„å•é¡Œã€‚

**ChatGPT å°è©±é€£çµ**: https://chatgpt.com/share/6982afab-3d98-800a-8ecb-b019b1802580
