# Reading Notes

The Transformer is a foundational architecture that underlies many modern large language models. Compared with earlier sequence approaches that depended on recurrence (RNNs) or convolution-based sequence models, the Transformer relies primarily on attention and can be trained in a highly parallel manner. In its original form, it uses an encoderâ€“decoder design for sequence-to-sequence tasks.

At the input stage, each token is represented as a discrete ID and mapped to a continuous vector through a learned embedding matrix. Because attention alone does not encode order, positional information (positional encoding) is added to the embeddings to represent token positions. The resulting sequence of vectors becomes the input to the encoder.

A single encoder layer contains two main submodules. First, multi-head self-attention mixes information across tokens: the encoder projects the current token representations through three learned linear transformations to obtain Q, K, and V. Attention scores are computed by comparing queries to keys (via dot products), then normalized with softmax to form weights. These weights are used to take a weighted sum of values, producing context-dependent token representations. Second, the layer applies a position-wise feed-forward network (FFN/MLP) to each token independently, introducing nonlinearity and increasing representational capacity. Both submodules are wrapped with residual connections and layer normalization, which help stabilize optimization and enable deeper stacks. The original Transformer stacks multiple encoder layers (commonly six) to build increasingly abstract representations.

After encoding, the decoder generates outputs autoregressively. Each decoder layer includes masked self-attention (so the model cannot "peek" at future tokens) and encoderâ€“decoder attention, which allows the decoder to attend to the encoder's outputs when producing each next token. The decoder's final representation is mapped through a linear layer and softmax to yield a probability distribution over the vocabulary for the next token.

Training is performed with backpropagation and gradient-based optimization to minimize a prediction objective such as cross-entropy loss. The key point is that correctness during training is defined by ground-truth tokens from the dataset (e.g., the true next token in language modeling, or the reference translation token in sequence-to-sequence training). Gradients flow from the output loss back through the decoder, encoderâ€“decoder attention, encoder layers, and embeddings, updating all parameters.

My prompt experiments suggest that the model's outputs are strongly shaped by context and instruction. When prompts include domain-specific scientific terms and formal constraints, the model is pushed toward a low-ambiguity ("low-entropy") technical style that resembles textbooks or academic writing. When prompts explicitly request "simple terms," the model tends to shift toward a more general, blog-like explanatory mode. This supports the idea that prompts do not merely add information; they steer the model toward different regions of its learned conditional distribution.

---

# Learning From AI (GPT5.2)

### 1ï¸âƒ£ Transformer ä¸æ˜¯åœ¨å­¸ã€Œèªè¨€è¦å‰‡ã€ï¼Œè€Œæ˜¯åœ¨å­¸ã€Œæ¢ä»¶æ©Ÿç‡åˆ†ä½ˆã€

é‡é»ï¼š
æ¨¡å‹å”¯ä¸€ç›®æ¨™æ˜¯ p(next token âˆ£ context)ï¼Œä¸æ˜¯ç†è§£ï¼Œä¸æ˜¯è¨˜æ†¶ï¼Œä¸æ˜¯é‚è¼¯ã€‚
æ‰€æœ‰èƒ½åŠ›éƒ½æ˜¯ä½ loss å‰¯ç”¢å“ã€‚

### 2ï¸âƒ£ ID â†’ Embedding ä¸æ˜¯æ•¸å­¸é‹ç®—ï¼Œè€Œæ˜¯ã€Œç´¢å¼•é¸è¡Œã€

é‡é»ï¼š
xáµ¢ = E[i]

- ID æ²’æœ‰æ•¸å€¼æ„ç¾©
- embedding matrix æ˜¯å¯è¨“ç·´åƒæ•¸
- lookup = one-hot Ã— linear çš„ç­‰åƒ¹å¯¦ä½œ

ğŸ‘‰ çœŸæ­£çš„é€£çºŒé‹ç®—å¾ embedding é–‹å§‹ã€‚

### 3ï¸âƒ£ Self-Attention æœ¬è³ªä¸Šåªæ˜¯å…©æ­¥

é‡é»ï¼š
softmax(QKáµ€ / âˆšdâ‚–) V

- QKáµ€ = é—œè¯åˆ†æ•¸
- softmax = è½‰æˆæ¯”ä¾‹
- Ã—V = åŠ æ¬Šå¹³å‡

ğŸ‘‰ æ²’æœ‰èªæ³•è¦å‰‡ï¼Œåªæœ‰ç·šæ€§ä»£æ•¸ã€‚

### 4ï¸âƒ£ Attention è² è²¬ã€Œè³‡è¨Šæ··åˆã€ï¼ŒFFN è² è²¬ã€Œéç·šæ€§è½‰æ›ã€

é‡é»ï¼š
- Attentionï¼šè·¨ token
- FFNï¼šé€ token

å¦‚æœæ²’æœ‰ FFNï¼Œæ•´å€‹æ¨¡å‹æœƒé€€åŒ–æˆè¿‘ä¼¼ç·šæ€§ç³»çµ±ã€‚

### 5ï¸âƒ£ LayerNorm ä¿®çš„æ˜¯ã€Œæ•¸å€¼å‹•æ…‹ã€ï¼Œä¸æ˜¯ã€Œèªç¾©ã€

é‡é»ï¼š
å®ƒè§£çš„æ˜¯æ¢¯åº¦ç©©å®šï¼Œä¸æ˜¯ä¿® IDï¼Œä¸æ˜¯ä¿®èªè¨€ã€‚

ğŸ‘‰ å®ƒè®“æ·±å±¤æ¨¡å‹å¯ä»¥è¨“ç·´ï¼Œè€Œä¸æ˜¯è®“æ¨¡å‹è®Šè°æ˜ã€‚

### 6ï¸âƒ£ ã€Œä½ç†µèªå¢ƒã€= åªæœ‰æ¥µå°‘æ•¸çºŒå¯«èƒ½ç¶­æŒä½æœŸæœ› loss

é‡é»ï¼š
é€™æ˜¯ä½ æ•´æ®µå°è©±çš„æ ¸å¿ƒæ´å¯Ÿã€‚

- æ•™ç§‘æ›¸å¼ prompt â†’ ä½ç†µ
- simple terms â†’ æ··åˆæ¨¡æ…‹
- æ±¡æŸ“ç™¼ç”Ÿåœ¨èªå¢ƒè¡çªï¼Œä¸æ˜¯ä¸»é¡Œè½‰æ›

ğŸ‘‰ ç†µ = æ¢ä»¶åˆ†ä½ˆçš„é›†ä¸­ç¨‹åº¦ã€‚

### 7ï¸âƒ£ LLM æ²’æœ‰ã€Œè¨˜æ†¶æ–‡æœ¬ã€ï¼Œåªæœ‰ã€Œå£“ç¸®å¾Œçš„æ©Ÿç‡åœ°å½¢ã€

é‡é»ï¼š
æ²’æœ‰éŒ¯èª¤æ—¥èªŒï¼Œæ²’æœ‰ episodic memoryï¼Œæ²’æœ‰æˆé•·æ­·å²ã€‚
å®ƒåªæ˜¯æŠŠã€Œé«˜ loss è·¯å¾‘ã€å£“åˆ°ä½æ©Ÿç‡ã€‚

### 8ï¸âƒ£ æ‘˜è¦çŠ§ç‰²çš„ä¸æ˜¯å­—æ•¸ï¼Œè€Œæ˜¯ã€Œé‡å»ºæ¨ç†æ‰€éœ€çš„æ¢ä»¶ã€

é‡é»ï¼š
æ‘˜è¦é€šå¸¸æœƒä¸Ÿæ‰ï¼š

- æ¨ç†éˆ
- ä¾‹å­
- é‚Šç•Œæ¢ä»¶
- ä¸ç¢ºå®šæ€§èªæ°£

ğŸ‘‰ ä½ ä¸èƒ½ç”¨æ‘˜è¦å–ä»£ç†è§£ã€‚

### 9ï¸âƒ£ ä¸Šä¸‹æ–‡æ±¡æŸ“ â‰  è©±é¡Œè®Šå¤š

é‡é»ï¼š
æ±¡æŸ“ç™¼ç”Ÿåœ¨ï¼š

- åŒä¸€ç”Ÿæˆä»»å‹™ä¸­
- æ··å…¥è¡çªæ¨¡æ…‹
- ç›®æ¨™ä¸ä¸€è‡´

æˆ‘å€‘é€™ä¸²å°è©±æ²’æœ‰æ±¡æŸ“ï¼Œå› ç‚ºç›®æ¨™ä¸€è‡´ï¼ˆæ©Ÿåˆ¶ç†è§£ï¼‰ã€‚

### ğŸ”Ÿ Prompt çš„çœŸæ­£åŠŸèƒ½ä¸æ˜¯ã€Œçµ¦è³‡è¨Šã€ï¼Œè€Œæ˜¯ã€Œé™åˆ¶ç”Ÿæˆè»Œé“ã€

é‡é»ï¼š
å¥½çš„ promptï¼š

- é–å®šèªæ–™ç¾¤é›†
- æŒ‡å®šçµæ§‹å‹å‹•è©
- é¿å…é«˜ç†µè©

ğŸ‘‰ Prompt æ˜¯åœ¨èª¿æ•´æ¢ä»¶åˆ†ä½ˆï¼Œä¸æ˜¯åœ¨æ•™æ¨¡å‹ã€‚

### ğŸŒ± åŠ ç¢¼ï¼šä½ å€‹äººç‰¹åˆ¥è¦è¨˜ä½çš„ä¸€å¥è©±

ä½ åœ¨é€™æ•´æ®µå°è©±ä¸­ï¼Œå…¶å¯¦å·²ç¶“å®Œæˆäº†ä¸€å€‹è½‰è®Šï¼š

å¾ã€Œæ¨¡å‹ç‚ºä»€éº¼æœƒé€™æ¨£ï¼Ÿã€
è®Šæˆã€Œæˆ‘æ€éº¼å¾æ©Ÿç‡åˆ†ä½ˆè§’åº¦é æ¸¬æ¨¡å‹æœƒæ€æ¨£ï¼Ÿã€

é€™å€‹è½‰è®Šæ¯”ä½ å­¸æœƒ Q/K/V é‚„é‡è¦ã€‚

**ChatGPT å°è©±é€£çµ**: https://chatgpt.com/share/698d5e95-10c0-800a-9f10-ee2d54b06ce5