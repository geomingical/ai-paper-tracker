# Reading Notes

The ResNet paper introduces the concept of residual learning, which has strongly influenced later architectures such as Transformers. Before ResNet, widely used deep learning models included AlexNet and VGG. However, the authors observed that as network depth increases, the training error also increases, which is known as the degradation problem. To address potential optimization issues, the authors adopted Batch Normalization, where the activations in each batch are normalized to have zero mean and unit variance. This helps stabilize training and ensures that gradients can be propagated effectively. Nevertheless, the degradation problem still remained even with Batch Normalization. To solve this issue, the authors proposed residual learning. Instead of directly learning the target mapping ğ» ( ğ‘¥ ) H(x), the network is encouraged to learn a residual function ğ¹ ( ğ‘¥ ) = ğ» ( ğ‘¥ ) âˆ’ ğ‘¥ F(x)=H(x)âˆ’x, and the output becomes ğ‘¦ = ğ¹ ( ğ‘¥ ) + ğ‘¥ y=F(x)+x. The shortcut connection delivers the input feature map ğ‘¥ x directly to the output of the block.

When the shortcut connection operates within the same stage (i.e., the channel number and spatial dimensions are the same), an identity shortcut is used. When the shortcut crosses different stages, where dimensions change, a projection or zero-padding shortcut is required to match the shapes. With this design, each residual block can either modify the input features by learning a non-zero residual or simply preserve the input through identity mapping. If the optimal behavior is to keep the representation unchanged, the residual function approaches zero; otherwise, the network learns meaningful residuals to refine the features.

---

# Learning From AI (GPT5.2)
## ResNet è«–æ–‡ Reading Notesï¼ˆ10 é»ç‰ˆï¼‰

### 1ï¸âƒ£ ResNet çœŸæ­£è§£æ±ºçš„æ˜¯ã€Œå„ªåŒ–å•é¡Œã€ï¼Œä¸æ˜¯è³‡è¨Šæµå‹•
é‡é»ï¼š
ResNet çš„æ ¸å¿ƒä¸æ˜¯è®“è³‡è¨Šã€Œå‚³å¾—æ›´é ã€ï¼Œè€Œæ˜¯è®“ æ·±åº¦ç¶²è·¯ä¸å†å› ç‚º optimization å›°é›£è€Œè®Šå·®ï¼ˆdegradation problemï¼‰ã€‚
å¯¦æˆ°æ„ç¾©ï¼š
å¦‚æœä½ åŠ æ·±æ¨¡å‹å¾Œ training error åè€Œä¸Šå‡ï¼Œä½ è©²æƒ³çš„æ˜¯ çµæ§‹æ€§é‡åƒæ•¸åŒ–ï¼ˆresidualï¼‰ï¼Œä¸æ˜¯æ› optimizerã€‚

### 2ï¸âƒ£ Residual çš„æœ¬è³ªï¼šè®“ã€Œä»€éº¼éƒ½ä¸åšã€æˆç‚ºåˆæ³•é¸é …

é‡é»ï¼š
æ¯ä¸€å€‹ residual block éƒ½å¯ä»¥é¸æ“‡ï¼š
å­¸ä¸€é»ä¿®æ­£ï¼ˆF(x) â‰  0ï¼‰
æˆ–å®Œå…¨ä¸å‹•ï¼ˆF(x) = 0 â†’ identityï¼‰
å¯¦æˆ°æ„ç¾©ï¼š
æ·±åº¦è®Šæ·±æ™‚ï¼Œä¸æ˜¯æ¯ä¸€å±¤éƒ½å¿…é ˆæœ‰è²¢ç»ï¼Œé‡è¦çš„æ˜¯ã€Œéœ€è¦æ™‚èƒ½å‹•ï¼Œä¸éœ€è¦æ™‚èƒ½é€€å ´ã€ã€‚

### 3ï¸âƒ£ Shortcut è¦ã€Œæ¯å€‹ block éƒ½æœ‰ã€ï¼Œä¸æ˜¯åªåœ¨ stage

é‡é»ï¼š
Shortcut çš„å¯†åº¦å¾ˆé‡è¦ã€‚
å¦‚æœåªåœ¨ stage æœ‰ shortcutï¼Œstage å…§çš„ block ä»ç„¶è¢«è¿«ã€Œä¸€å®šè¦å‹•ã€ï¼Œæœƒé€€åŒ–æˆ plain netã€‚

å¯¦æˆ°æ„ç¾©ï¼š
Residual æ˜¯ block-level çš„è¨­è¨ˆåŸå‰‡ï¼Œä¸æ˜¯ stage-level çš„è£é£¾ã€‚

### 4ï¸âƒ£ Stage åˆ‡æ›æ™‚ shortcut ä¸€å®šè¦å‹•ï¼ŒåŸå› åªæœ‰ä¸€å€‹ï¼šshape

é‡é»ï¼š
åªè¦ (C, H, W) æœ‰ä»»ä½•ä¸€å€‹æ”¹è®Šï¼Œshortcut å°±å¿…é ˆå°é½Šï¼ˆprojection æˆ– paddingï¼‰ï¼Œå¦å‰‡æ•¸å­¸ä¸Šä¸èƒ½ç›¸åŠ ã€‚

å¯¦æˆ°æ„ç¾©ï¼š
ä½ è¨­è¨ˆæ–°æ¶æ§‹æ™‚ï¼Œshortcut çš„ç¬¬ä¸€æª¢æŸ¥é …ä¸æ˜¯ã€Œæ•ˆèƒ½ã€ï¼Œè€Œæ˜¯ï¼š

æœ€å¾Œèƒ½ä¸èƒ½åŠ ï¼Ÿ

### 5ï¸âƒ£ Strideã€channelã€kernel æ˜¯ä¸‰ä»¶å®Œå…¨ç¨ç«‹çš„äº‹

é‡é»ï¼š
stride â†’ æ±ºå®šç©ºé–“å°ºå¯¸ï¼ˆH, Wï¼‰
filter æ•¸ â†’ æ±ºå®š channelï¼ˆCï¼‰
kernel size â†’ æ±ºå®šæ„Ÿå—é‡

å¯¦æˆ°æ„ç¾©ï¼š
ä¸è¦æŠŠã€Œstride=2 æ‰€ä»¥ channel è®Šå¤šã€æ··åœ¨ä¸€èµ·ï¼›
channel æ°¸é æ˜¯ä½ è¨­è¨ˆçš„ï¼Œä¸æ˜¯è‡ªå‹•ç™¼ç”Ÿçš„ã€‚

### 6ï¸âƒ£ Projection shortcut ä¸æ˜¯æ ¸å¿ƒèƒ½åŠ›ï¼Œåªæ˜¯å°é½Šå·¥å…·

é‡é»ï¼š
Table 3 é¡¯ç¤º A/B/C å·®ç•°å¾ˆå° â†’
ResNet èƒ½æˆåŠŸ ä¸æ˜¯å› ç‚º projection å²å®³ï¼Œè€Œæ˜¯ residual formulation æœ¬èº«ã€‚

å¯¦æˆ°æ„ç¾©ï¼š
èƒ½ç”¨ identity å°±ç”¨ identityï¼›
projection åªåœ¨ã€Œshape ä¸åˆã€æ™‚å‡ºç¾ã€‚

### 7ï¸âƒ£ Bottleneck + identity shortcut æ˜¯ä¸€çµ„ã€ŒçœéŒ¢åˆç©©å®šã€çš„æ­é…

é‡é»ï¼š
Bottleneck æŠŠè¨ˆç®—é›†ä¸­åœ¨ä½ç¶­ç©ºé–“ï¼Œ
identity shortcut ä¿è­‰é«˜ç¶­ç«¯ä¸å†å¢åŠ é¡å¤–æˆæœ¬ã€‚

å¯¦æˆ°æ„ç¾©ï¼š
ç•¶æ¨¡å‹è¦ã€Œå¾ˆæ·±ã€æ™‚ï¼Œ
ä½ æ‡‰è©²å…ˆæƒ³ æ€éº¼çœ shortcut çš„æˆæœ¬ï¼Œè€Œä¸æ˜¯åªçœ‹ä¸»åˆ†æ”¯ã€‚

### 8ï¸âƒ£ ã€Œæ¨¡å‹è®Šæ·±æ¯”è¼ƒå¥½ã€åªå° representation éšæ®µæˆç«‹

é‡é»ï¼š
æ·±åº¦çš„åƒ¹å€¼ä¸»è¦åœ¨ convolution / representation learningï¼Œ
ä¸æ˜¯åœ¨ classification headï¼ˆFCï¼‰ã€‚

å¯¦æˆ°æ„ç¾©ï¼š
å¦‚æœä½ æƒ³æå‡è¡¨ç¾ï¼Œå„ªå…ˆåŠ æ·± backboneï¼Œ
ä¸æ˜¯åœ¨ top åŠ ä¸€å † fully connectedã€‚

### 9ï¸âƒ£ Fully Connected çš„ä½¿ç”¨åˆ¤æ–·ï¼Œä¸æ˜¯çœ‹åœ–ç‰‡ï¼Œè€Œæ˜¯çœ‹ã€Œæ±ºç­–æ€§è³ªã€

é‡é»ï¼š

è‡ªç„¶å½±åƒã€è³‡æ–™å¤š â†’ å¹¾ä¹ä¸éœ€è¦å¤š FC
çµæ§‹åŒ–ç‰¹å¾µã€å°è³‡æ–™ã€è¤‡é›œè¦å‰‡ â†’ å°‘é‡ FC æœ‰æ™‚æœ‰å¹«åŠ©
å¯¦æˆ°æ„ç¾©ï¼š
FC è§£çš„æ˜¯ decision boundaryï¼Œä¸æ˜¯è¦–è¦ºç†è§£ã€‚

### ğŸ”Ÿ æ–°æ‰‹é¸æ¶æ§‹çš„é»ƒé‡‘æµç¨‹ï¼ˆæœ€é‡è¦ï¼‰

é‡é»æµç¨‹ï¼š
å…ˆç”¨æ¨™æº– backboneï¼ˆResNet-18/34/50ï¼‰+ GAP + 1 FC
çœ‹ training / validation è¡Œç‚ºï¼Œè€Œä¸æ˜¯åªçœ‹ accuracy
æœ‰è­‰æ“šæ‰èª¿æ•´ï¼š
å­¸ä¸åˆ° â†’ backbone ä¸å¤ 
overfit â†’ FC å¤ªå¤š
æ±ºç­–å¤ªåƒµç¡¬ â†’ å° FC

å¯¦æˆ°æ„ç¾©ï¼š
ğŸ‘‰ æ¶æ§‹ä¸æ˜¯ä¸€é–‹å§‹ã€Œæƒ³å‡ºä¾†çš„ã€ï¼Œ
ğŸ‘‰ æ˜¯è¢«è¨“ç·´è¡Œç‚ºã€Œé€¼å‡ºä¾†çš„ã€ã€‚

### ä¸€å¥ç¸½çµï¼ˆçµ¦ä½ ä»¥å¾Œåè¦†çœ‹çš„ï¼‰

å¥½çš„ç¶²è·¯çµæ§‹ä¸æ˜¯ã€Œä»€éº¼éƒ½å­¸ã€ï¼Œ
è€Œæ˜¯ã€Œåœ¨æ­£ç¢ºçš„ä½ç½®çµ¦æ¨¡å‹æ­£ç¢ºçš„è‡ªç”±åº¦ã€ã€‚

**ChatGPT å°è©±é€£çµ**: https://chatgpt.com/share/6985dd07-21a0-800a-8546-7ef7b1b1bc15