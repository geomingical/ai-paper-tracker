# GPT-3: Language Models are Few-Shot Learners - Notes

## The Big Picture

GPT-3 demonstrated that scaling language models to 175B parameters unlocks remarkable few-shot learning capabilities without task-specific fine-tuning.

## Key Findings

### In-Context Learning
- Few-shot: provide a few examples in the prompt
- One-shot: single example
- Zero-shot: just task description

The model learns to pattern-match from context without updating weights.

### Scaling Effects
- Performance scales smoothly with model size
- Emergent capabilities appear at larger scales
- 175B parameters, 300B tokens training data

## Critical Observations

1. **Task-agnostic**: Same model, same weights, many tasks
2. **Prompt engineering matters**: How you ask affects results
3. **Limitations**: Still struggles with reasoning, arithmetic, factual consistency

## Historical Significance

This paper kicked off the "scaling era" and directly led to ChatGPT and the current LLM revolution.
