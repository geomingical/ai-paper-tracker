# Reading Notes

AlexNet is widely regarded as a turning point that demonstrated the practical power of deep convolutional neural networks (DNNs) at large scale. Before AlexNet, many image recognition systems relied on hand-crafted features designed for specific image categories or tasks, and then applied shallow classifiers such as SVMs or logistic regression for classification.

In contrast, AlexNet used a deep architecture with five convolutional layers and three fully connected layers. The earlier layers tend to learn low-level visual patterns such as edges and textures, while deeper layers combine these patterns into more abstract, high-level representations. Trained end-to-end with backpropagation, along with techniques such as ReLU and regularization methods, the model achieved a substantial performance improvement on ImageNet-style benchmarks.

In addition, the bookã€Šæ·±åº¦å­¸ç¿’é©å‘½ï¼šå¾æ­·å²åˆ°æœªä¾†ã€‹addresses the history of deep learning, including how major technology companies pursued and invested in the field about a decade ago.

**Reference**: ã€Šæ·±åº¦å­¸ç¿’é©å‘½ï¼šå¾æ­·å²åˆ°æœªä¾†ã€‹ https://www.tenlong.com.tw/products/9787521747553

---

# Learning From AI (GPT5.2)

## AlexNetï¼ˆ2012ï¼‰â€” 10 é»å¿…è¨˜ Learning Notes

### 1ï¸âƒ£ å•é¡Œè¦æ¨¡æœ¬èº«å°±æ˜¯çªç ´çš„ä¸€åŠ

ImageNetï¼ˆ100 è¬å¼µã€1000 é¡ï¼‰ä¸æ˜¯èƒŒæ™¯ï¼Œè€Œæ˜¯æ ¸å¿ƒæŒ‘æˆ°ã€‚åœ¨é€™å€‹è¦æ¨¡ä¸‹ï¼Œæ‰‹å·¥ç‰¹å¾µ + å‚³çµ±åˆ†é¡å™¨å…¨é¢å¤±æ•ˆã€‚

ä¾æ“šï¼šSection 1, Section 2

ğŸ‘‰ è¨˜ä½ï¼šAlexNet è§£çš„ä¸æ˜¯ã€Œæ›´æº–çš„æ¨¡å‹ã€ï¼Œè€Œæ˜¯ã€Œåœ¨é€™å€‹è¦æ¨¡ä¸‹é‚„èƒ½å­¸å¾—å‹•ã€ã€‚

### 2ï¸âƒ£ CNN çš„æ ¸å¿ƒä¸æ˜¯ convolutionï¼Œè€Œæ˜¯ã€Œç«¯åˆ°ç«¯å­¸ç‰¹å¾µã€

å·ç©ä¸æ˜¯æ–°æ±è¥¿ï¼Œæ–°çš„æ˜¯ï¼šå¾ pixel åˆ° classï¼Œæ‰€æœ‰è¡¨ç¤ºéƒ½ç‚ºåŒä¸€å€‹ loss æœå‹™ã€‚

ä¾æ“šï¼šAbstract, Section 1

ğŸ‘‰ é€™æ˜¯ AlexNet èˆ‡ SIFT/HOG æ™‚ä»£çš„æ ¹æœ¬åˆ†æ°´å¶ºã€‚

### 3ï¸âƒ£ Channelï¼ˆåšåº¦ï¼‰â‰  é¡è‰²ï¼Œè€Œæ˜¯ã€Œç‰¹å¾µç¨®é¡æ•¸ã€

- C1 çš„ 96
- C2 çš„ 256
- C3/C4 çš„ 384

éƒ½æ˜¯ kernel æ•¸é‡ = ä½œè€…è¨­å®šçš„è¶…åƒæ•¸ã€‚

ä¾æ“šï¼šSection 3.5, Figure 2

ğŸ‘‰ ç©ºé–“å°ºå¯¸å›ç­”ã€Œåœ¨å“ªè£¡ã€ï¼Œchannel å›ç­”ã€Œç”¨å¹¾ç¨®æ–¹å¼çœ‹ã€ã€‚

### 4ï¸âƒ£ æ»‘å‹•ï¼ˆstrideï¼‰åªå½±éŸ¿ 2D å°ºå¯¸ï¼Œä¸æœƒç”¢ç”Ÿåšåº¦

- 55Ã—55 ä¾†è‡ª stride = 4
- 96 ä¾†è‡ª 96 å€‹ kernel

ä¾æ“šï¼šSection 3.5

ğŸ‘‰ åšåº¦ä¸æ˜¯ç–Šä½ç½®ï¼Œæ˜¯ç–Šã€Œä¸åŒçš„å›æ‡‰ã€ã€‚

### 5ï¸âƒ£ ReLU çš„æ„ç¾©æ˜¯ã€Œè®“æ·±ç¶²çœŸçš„å­¸å¾—å‹•ã€

ReLU ä¸æ˜¯ç”Ÿç‰©æ¨¡æ“¬ï¼Œè€Œæ˜¯æ•¸å€¼å·¥ç¨‹ï¼š
- ä¸é£½å’Œ
- æ¢¯åº¦ä¸å®¹æ˜“æ¶ˆå¤±
- è¨“ç·´é€Ÿåº¦å¤§å¹…æå‡

ä¾æ“šï¼šSection 3.1, Figure 1

ğŸ‘‰ æ²’æœ‰ ReLUï¼ŒAlexNet çš„æ·±åº¦åœ¨ç•¶å¹´å¹¾ä¹ä¸å¯è¡Œã€‚

### 6ï¸âƒ£ Data Augmentation æ˜¯ã€Œå­¸ä¸è®Šæ€§ã€ï¼Œä¸æ˜¯è³‡æ–™ä½œå¼Š

AlexNet åªåšå…©ä»¶äº‹ï¼š
- éš¨æ©Ÿè£åˆ‡ + ç¿»è½‰ï¼ˆç©ºé–“ä¸è®Šæ€§ï¼‰
- PCA-based é¡è‰²æ“¾å‹•ï¼ˆå…‰ç…§ä¸è®Šæ€§ï¼‰

ä¾æ“šï¼šSection 4.1

ğŸ‘‰ æ¨¡å‹ä¸æ˜¯è¨˜ä½åœ–ç‰‡ï¼Œè€Œæ˜¯è¢«é€¼å¿½ç•¥ä¸é‡è¦çš„è®ŠåŒ–ã€‚

### 7ï¸âƒ£ Dropout ä¸æ˜¯é—œè³‡æ–™ï¼Œä¹Ÿä¸æ˜¯é—œé¡åˆ¥

Dropout = è¨“ç·´æ™‚ï¼Œéš¨æ©Ÿè®“ã€Œä¸­é–“ neuron æš«æ™‚å¤±è²ã€
- ç”¨åœ¨ FC6ã€FC7
- ä¸ç”¨åœ¨æœ€å¾Œ softmax

ä¾æ“šï¼šSection 4.2

ğŸ‘‰ é˜²æ­¢ co-adaptationï¼Œè€Œä¸æ˜¯é™ä½ç¶­åº¦ã€‚

### 8ï¸âƒ£ Flatten æ˜¯ã€Œè³‡è¨Šé‡æ’ã€ï¼Œä¸æ˜¯é‹ç®—

13Ã—13Ã—256 â†’ 43264

åªæ˜¯æŠŠä¸‰ç¶­ tensor æ‹‰æˆä¸€æ¢å‘é‡ã€‚æ²’æœ‰å­¸ç¿’ã€æ²’æœ‰æ¬Šé‡ã€æ²’æœ‰ä¿ç•™ç©ºé–“é—œä¿‚ã€‚

ä¾æ“šï¼šSection 3.5

ğŸ‘‰ å¾é€™ä¸€åˆ»é–‹å§‹ï¼Œè¡¨ç¤ºä¸å†å°é½Šå½±åƒåº§æ¨™ã€‚

### 9ï¸âƒ£ 1000 å€‹è¼¸å‡ºä¹‹æ‰€ä»¥å°æ‡‰ 1000 å€‹é¡åˆ¥ï¼Œæ˜¯ loss åœ¨ã€Œé»åã€

- forwardï¼šæ¨¡å‹ä¸çŸ¥é“å“ªå€‹ index æ˜¯å“ªå€‹é¡
- lossï¼šåªæ‡²ç½°æ­£ç¢º label å°æ‡‰çš„é‚£ä¸€å€‹ä½ç½®
- backpropï¼šåè¦†æŠŠèªç¾©å£“åˆ°å›ºå®š index

ä¾æ“šï¼šSection 3.5

ğŸ‘‰ èªç¾©ä¸æ˜¯ç®—å‡ºä¾†çš„ï¼Œæ˜¯è¢«åå‘å‚³æ’­ã€Œå›ºå®šä½çš„ã€ã€‚

### ğŸ”Ÿ AlexNet çš„æˆåŠŸæ˜¯ã€Œå·¥ç¨‹æŠ˜è¡·ã€ï¼Œä¸æ˜¯ç†è«–æœ€å„ª

- å¤§ kernelï¼ˆ11Ã—11ï¼‰
- GPU åˆ†çµ„
- æ‰‹å‹•è¨­è¨ˆ channel æ•¸

éƒ½æ˜¯ 2012 å¹´ç¡¬é«”èˆ‡ç¶“é©—çš„çµæœã€‚

ä¾æ“šï¼šSection 3.2, Section 7

ğŸ‘‰ ä½ è¦å­¸çš„æ˜¯ã€Œè¨­è¨ˆåŸå‰‡ã€ï¼Œä¸æ˜¯ç…§æŠ„æ•¸å­—ã€‚

**ChatGPT å°è©±é€£çµ**: https://chatgpt.com/share/697f3a95-687c-800a-9bfa-2f067b4d7200
